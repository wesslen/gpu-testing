{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNdkK1IbewrcHcG5oj1nXRT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/gpu-testing/blob/main/notebooks/transformers_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tknp1IyeYnMm",
        "outputId": "0db409fa-e3ed-4e9a-8782-f6bee05d1d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 50 iterations...\n",
            "Iteration 10/50\n",
            "Iteration 20/50\n",
            "Iteration 30/50\n",
            "Iteration 40/50\n",
            "Iteration 50/50\n",
            "\n",
            "Results:\n",
            "Average latency: 106.65 ms\n",
            "Median latency: 106.64 ms\n",
            "90th percentile latency: 106.71 ms\n",
            "99th percentile latency: 106.76 ms\n",
            "Throughput: 153623.44 tokens/second\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "class BenchmarkConfig(PretrainedConfig):\n",
        "    model_type = \"benchmark\"\n",
        "    def __init__(self, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, **kwargs):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "class BenchmarkModel(PreTrainedModel):\n",
        "    config_class = BenchmarkConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            torch.nn.Sequential(\n",
        "                torch.nn.Linear(config.hidden_size, config.hidden_size * 4),\n",
        "                torch.nn.GELU(),\n",
        "                torch.nn.Linear(config.hidden_size * 4, config.hidden_size),\n",
        "                torch.nn.LayerNorm(config.hidden_size)\n",
        "            ) for _ in range(config.num_hidden_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x) + x\n",
        "        return x\n",
        "\n",
        "def run_benchmark(batch_size=32, seq_length=512, num_warmup=10, num_iterations=50):\n",
        "    # Configure device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Running on device: {device}\")\n",
        "    if device.type == \"cuda\":\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "    # Initialize model\n",
        "    config = BenchmarkConfig()\n",
        "    model = BenchmarkModel(config).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy input\n",
        "    dummy_input = torch.randn(batch_size, seq_length, config.hidden_size, device=device)\n",
        "\n",
        "    # Warmup\n",
        "    print(\"Warming up...\")\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmup):\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    # Benchmark\n",
        "    print(f\"Running benchmark with {num_iterations} iterations...\")\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        torch.cuda.synchronize()\n",
        "        for i in range(num_iterations):\n",
        "            start = time()\n",
        "            _ = model(dummy_input)\n",
        "            torch.cuda.synchronize()\n",
        "            end = time()\n",
        "            times.append(end - start)\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Iteration {i + 1}/{num_iterations}\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    times = np.array(times) * 1000  # Convert to milliseconds\n",
        "    stats = {\n",
        "        'avg_latency': np.mean(times),\n",
        "        'median_latency': np.median(times),\n",
        "        'p90_latency': np.percentile(times, 90),\n",
        "        'p99_latency': np.percentile(times, 99),\n",
        "        'throughput': batch_size * seq_length / (np.mean(times) / 1000)\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Average latency: {stats['avg_latency']:.2f} ms\")\n",
        "    print(f\"Median latency: {stats['median_latency']:.2f} ms\")\n",
        "    print(f\"90th percentile latency: {stats['p90_latency']:.2f} ms\")\n",
        "    print(f\"99th percentile latency: {stats['p99_latency']:.2f} ms\")\n",
        "    print(f\"Throughput: {stats['throughput']:.2f} tokens/second\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests\n",
        "\n",
        "- **Batch Size Testing** (1 to 64 samples): Tests how many examples your GPU can process at once. This is crucial for training deep learning models - larger batch sizes can speed up training, but only if your GPU has enough memory and computational power. Understanding your GPU's optimal batch size helps you maximize training efficiency without running out of memory.\n",
        "- **Sequence Length Impact (128 to 2048 tokens)**: Measures how your GPU handles different text lengths. This is vital for NLP tasks - longer sequences (like full documents) require more memory and computation than shorter ones (like sentences). Knowing these limits helps you design efficient text processing pipelines and choose appropriate text chunking strategies.\n",
        "- **Throughput vs Latency Trade-offs**: The benchmark measures both how many tokens per second your GPU can process (throughput) and how long each batch takes (latency). This helps you balance between speed and responsiveness - crucial for deciding between batch processing (like training) and real-time applications (like inference in production)."
      ],
      "metadata": {
        "id": "b8-WpI2xbPOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "def run_benchmark_matrix():\n",
        "    # Test scenarios\n",
        "    scenarios = {\n",
        "        'batch_sizes': [1, 8, 32, 64],\n",
        "        'seq_lengths': [128, 512, 1024, 2048],\n",
        "        'num_warmup': 5,\n",
        "        'num_iterations': 20\n",
        "    }\n",
        "\n",
        "    # Create results directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_dir = f\"benchmark_results_{timestamp}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Store all results\n",
        "    results = []\n",
        "\n",
        "    # Run all combinations\n",
        "    total_runs = len(scenarios['batch_sizes']) * len(scenarios['seq_lengths'])\n",
        "    current_run = 0\n",
        "\n",
        "    print(f\"Starting benchmark matrix with {total_runs} combinations...\")\n",
        "    print(\"Configuration:\", json.dumps(scenarios, indent=2))\n",
        "\n",
        "    for batch_size, seq_length in itertools.product(\n",
        "        scenarios['batch_sizes'],\n",
        "        scenarios['seq_lengths']\n",
        "    ):\n",
        "        current_run += 1\n",
        "        print(f\"\\nRun {current_run}/{total_runs}\")\n",
        "        print(f\"Testing batch_size={batch_size}, seq_length={seq_length}\")\n",
        "\n",
        "        try:\n",
        "            # Run the benchmark\n",
        "            stats = run_benchmark(\n",
        "                batch_size=batch_size,\n",
        "                seq_length=seq_length,\n",
        "                num_warmup=scenarios['num_warmup'],\n",
        "                num_iterations=scenarios['num_iterations']\n",
        "            )\n",
        "\n",
        "            # Add configuration to results\n",
        "            result = {\n",
        "                'batch_size': batch_size,\n",
        "                'seq_length': seq_length,\n",
        "                'tokens_per_batch': batch_size * seq_length,\n",
        "                **stats\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error running benchmark with batch_size={batch_size}, \"\n",
        "                  f\"seq_length={seq_length}: {str(e)}\")\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Save raw results\n",
        "    df.to_csv(f\"{results_dir}/raw_results.csv\", index=False)\n",
        "\n",
        "    # Create pivot tables for different metrics\n",
        "    metrics = ['avg_latency', 'throughput']\n",
        "    for metric in metrics:\n",
        "        pivot = df.pivot(\n",
        "            index='batch_size',\n",
        "            columns='seq_length',\n",
        "            values=metric\n",
        "        )\n",
        "        pivot.to_csv(f\"{results_dir}/{metric}_matrix.csv\")\n",
        "\n",
        "        print(f\"\\n{metric.replace('_', ' ').title()} Matrix:\")\n",
        "        print(pivot)\n",
        "\n",
        "    # Save test configuration\n",
        "    with open(f\"{results_dir}/config.json\", 'w') as f:\n",
        "        json.dump(scenarios, f, indent=2)\n",
        "\n",
        "    print(f\"\\nBenchmark results saved in: {results_dir}\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results_df = run_benchmark_matrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rI4in5kae01",
        "outputId": "dca14c50-b26c-404d-f91c-9ea8a8f8d764"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting benchmark matrix with 16 combinations...\n",
            "Configuration: {\n",
            "  \"batch_sizes\": [\n",
            "    1,\n",
            "    8,\n",
            "    32,\n",
            "    64\n",
            "  ],\n",
            "  \"seq_lengths\": [\n",
            "    128,\n",
            "    512,\n",
            "    1024,\n",
            "    2048\n",
            "  ],\n",
            "  \"num_warmup\": 5,\n",
            "  \"num_iterations\": 20\n",
            "}\n",
            "\n",
            "Run 1/16\n",
            "Testing batch_size=1, seq_length=128\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 2.06 ms\n",
            "Median latency: 2.03 ms\n",
            "90th percentile latency: 2.04 ms\n",
            "99th percentile latency: 2.52 ms\n",
            "Throughput: 62114.47 tokens/second\n",
            "\n",
            "Run 2/16\n",
            "Testing batch_size=1, seq_length=512\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 5.11 ms\n",
            "Median latency: 5.54 ms\n",
            "90th percentile latency: 5.61 ms\n",
            "99th percentile latency: 5.76 ms\n",
            "Throughput: 100248.05 tokens/second\n",
            "\n",
            "Run 3/16\n",
            "Testing batch_size=1, seq_length=1024\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 8.20 ms\n",
            "Median latency: 8.19 ms\n",
            "90th percentile latency: 8.21 ms\n",
            "99th percentile latency: 8.32 ms\n",
            "Throughput: 124908.17 tokens/second\n",
            "\n",
            "Run 4/16\n",
            "Testing batch_size=1, seq_length=2048\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 15.53 ms\n",
            "Median latency: 15.52 ms\n",
            "90th percentile latency: 15.57 ms\n",
            "99th percentile latency: 15.58 ms\n",
            "Throughput: 131911.95 tokens/second\n",
            "\n",
            "Run 5/16\n",
            "Testing batch_size=8, seq_length=128\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 8.19 ms\n",
            "Median latency: 8.18 ms\n",
            "90th percentile latency: 8.21 ms\n",
            "99th percentile latency: 8.30 ms\n",
            "Throughput: 125007.96 tokens/second\n",
            "\n",
            "Run 6/16\n",
            "Testing batch_size=8, seq_length=512\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 28.53 ms\n",
            "Median latency: 28.52 ms\n",
            "90th percentile latency: 28.55 ms\n",
            "99th percentile latency: 28.57 ms\n",
            "Throughput: 143586.46 tokens/second\n",
            "\n",
            "Run 7/16\n",
            "Testing batch_size=8, seq_length=1024\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 55.55 ms\n",
            "Median latency: 55.53 ms\n",
            "90th percentile latency: 55.65 ms\n",
            "99th percentile latency: 55.66 ms\n",
            "Throughput: 147470.45 tokens/second\n",
            "\n",
            "Run 8/16\n",
            "Testing batch_size=8, seq_length=2048\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 106.58 ms\n",
            "Median latency: 106.55 ms\n",
            "90th percentile latency: 106.67 ms\n",
            "99th percentile latency: 106.75 ms\n",
            "Throughput: 153721.90 tokens/second\n",
            "\n",
            "Run 9/16\n",
            "Testing batch_size=32, seq_length=128\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 28.50 ms\n",
            "Median latency: 28.50 ms\n",
            "90th percentile latency: 28.52 ms\n",
            "99th percentile latency: 28.54 ms\n",
            "Throughput: 143707.17 tokens/second\n",
            "\n",
            "Run 10/16\n",
            "Testing batch_size=32, seq_length=512\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 106.60 ms\n",
            "Median latency: 106.57 ms\n",
            "90th percentile latency: 106.76 ms\n",
            "99th percentile latency: 106.78 ms\n",
            "Throughput: 153700.82 tokens/second\n",
            "\n",
            "Run 11/16\n",
            "Testing batch_size=32, seq_length=1024\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 208.91 ms\n",
            "Median latency: 208.90 ms\n",
            "90th percentile latency: 208.97 ms\n",
            "99th percentile latency: 209.01 ms\n",
            "Throughput: 156854.59 tokens/second\n",
            "\n",
            "Run 12/16\n",
            "Testing batch_size=32, seq_length=2048\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 413.58 ms\n",
            "Median latency: 413.58 ms\n",
            "90th percentile latency: 413.62 ms\n",
            "99th percentile latency: 413.63 ms\n",
            "Throughput: 158459.89 tokens/second\n",
            "\n",
            "Run 13/16\n",
            "Testing batch_size=64, seq_length=128\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 55.47 ms\n",
            "Median latency: 55.46 ms\n",
            "90th percentile latency: 55.50 ms\n",
            "99th percentile latency: 55.50 ms\n",
            "Throughput: 147691.74 tokens/second\n",
            "\n",
            "Run 14/16\n",
            "Testing batch_size=64, seq_length=512\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 208.87 ms\n",
            "Median latency: 208.87 ms\n",
            "90th percentile latency: 208.94 ms\n",
            "99th percentile latency: 208.95 ms\n",
            "Throughput: 156880.89 tokens/second\n",
            "\n",
            "Run 15/16\n",
            "Testing batch_size=64, seq_length=1024\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 413.52 ms\n",
            "Median latency: 413.52 ms\n",
            "90th percentile latency: 413.57 ms\n",
            "99th percentile latency: 413.60 ms\n",
            "Throughput: 158483.02 tokens/second\n",
            "\n",
            "Run 16/16\n",
            "Testing batch_size=64, seq_length=2048\n",
            "Running on device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Version: 12.1\n",
            "Warming up...\n",
            "Running benchmark with 20 iterations...\n",
            "Iteration 10/20\n",
            "Iteration 20/20\n",
            "\n",
            "Results:\n",
            "Average latency: 826.14 ms\n",
            "Median latency: 826.09 ms\n",
            "90th percentile latency: 826.28 ms\n",
            "99th percentile latency: 826.94 ms\n",
            "Throughput: 158656.85 tokens/second\n",
            "\n",
            "Avg Latency Matrix:\n",
            "seq_length       128         512         1024        2048\n",
            "batch_size                                               \n",
            "1            2.060711    5.107331    8.198023   15.525508\n",
            "8            8.191478   28.526366   55.550110  106.582081\n",
            "32          28.502405  106.596696  208.906865  413.581014\n",
            "64          55.466878  208.871841  413.520634  826.135170\n",
            "\n",
            "Throughput Matrix:\n",
            "seq_length           128            512            1024           2048\n",
            "batch_size                                                            \n",
            "1            62114.472218  100248.049781  124908.166235  131911.948365\n",
            "8           125007.961743  143586.464357  147470.454151  153721.899567\n",
            "32          143707.172941  153700.823355  156854.586761  158459.885324\n",
            "64          147691.743870  156880.888183  158483.022572  158656.845377\n",
            "\n",
            "Benchmark results saved in: benchmark_results_20241104_134449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now check versions\n",
        "import pkg_resources\n",
        "import sys\n",
        "\n",
        "def get_package_details():\n",
        "    \"\"\"Print details of specific packages and Python version\"\"\"\n",
        "    packages_to_check = [\n",
        "        'torch',\n",
        "        'transformers',\n",
        "        'numpy',\n",
        "        'sentencepiece'  # Often used by transformers\n",
        "    ]\n",
        "\n",
        "    print(\"Python version:\", sys.version.split()[0])\n",
        "    print(\"\\nPackage versions:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for package in packages_to_check:\n",
        "        try:\n",
        "            version = pkg_resources.get_distribution(package).version\n",
        "            print(f\"{package:<15} {version}\")\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            print(f\"{package:<15} Not installed\")\n",
        "\n",
        "# Check CUDA availability for PyTorch\n",
        "import torch\n",
        "print(\"\\nCUDA Status:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Current GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "# Run the check\n",
        "get_package_details()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eEJsKYsY2zI",
        "outputId": "69aa238c-f9c5-4b12-f72a-20f0b45a9fd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA Status:\n",
            "--------------------------------------------------\n",
            "CUDA available: True\n",
            "CUDA version: 12.1\n",
            "Current GPU: NVIDIA A100-SXM4-40GB\n",
            "Python version: 3.10.12\n",
            "\n",
            "Package versions:\n",
            "--------------------------------------------------\n",
            "torch           2.5.0+cu121\n",
            "transformers    4.44.2\n",
            "numpy           1.26.4\n",
            "sentencepiece   0.2.0\n"
          ]
        }
      ]
    }
  ]
}